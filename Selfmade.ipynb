{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e7ff73",
   "metadata": {},
   "source": [
    "# EEG Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e244f1b5",
   "metadata": {},
   "source": [
    "## Converting Files to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e3d0462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined CSV: ./csv_output/Person1Recording1.csv\n",
      "Saved combined CSV: ./csv_output/Person1Recording2.csv\n",
      "Saved combined CSV: ./csv_output/Person1Recording3.csv\n",
      "Saved combined CSV: ./csv_output/Person1Recording3.csv\n",
      "Saved combined CSV: ./csv_output/Person2Recording1.csv\n",
      "Saved combined CSV: ./csv_output/Person2Recording2.csv\n",
      "Saved combined CSV: ./csv_output/Person2Recording3.csv\n",
      "Saved combined CSV: ./csv_output/Person3Recording1.csv\n",
      "Saved combined CSV: ./csv_output/Person3Recording2.csv\n",
      "Saved combined CSV: ./csv_output/Person3Recording3.csv\n",
      "Saved combined CSV: ./csv_output/Person4Recording1.csv\n",
      "Saved combined CSV: ./csv_output/Person4Recording2.csv\n",
      "Saved combined CSV: ./csv_output/Person5Recording4.csv\n",
      "Saved combined CSV: ./csv_output/Person6Recording4.csv\n",
      "Saved combined CSV: ./csv_output/Person6Recording5.csv\n",
      "Saved combined CSV: ./csv_output/Person6Recording6.csv\n",
      "Saved combined CSV: ./csv_output/Person7Recording7.csv\n",
      "Saved combined CSV: ./csv_output/Person8Recording8.csv\n"
     ]
    }
   ],
   "source": [
    "# Notebook: EEG File Converter to CSV with Integrated Annotations\n",
    "import os\n",
    "import mne\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress MNE warnings\n",
    "mne.set_log_level('ERROR')\n",
    "\n",
    "# Define paths\n",
    "root_path = './data/selfmade_dataset/'       # adjust if necessary\n",
    "output_path = './csv_output/'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Channels to mark as misc\n",
    "misc_chs = ['Aux1', 'Aux2', 'x_dir', 'y_dir', 'z_dir']\n",
    "\n",
    "# Iterate over Person and Recording folders\n",
    "for person_dir in sorted(os.listdir(root_path)):\n",
    "    person_path = os.path.join(root_path, person_dir)\n",
    "    if not os.path.isdir(person_path):\n",
    "        continue\n",
    "\n",
    "    for recording_dir in sorted(os.listdir(person_path)):\n",
    "        recording_path = os.path.join(person_path, recording_dir)\n",
    "        if not os.path.isdir(recording_path):\n",
    "            continue\n",
    "\n",
    "        # Process .vhdr files\n",
    "        for file in sorted(os.listdir(recording_path)):\n",
    "            if not file.endswith('.vhdr'):\n",
    "                continue\n",
    "\n",
    "            vhdr_path = os.path.join(recording_path, file)\n",
    "            basename = file[:-5]  # strip .vhdr\n",
    "\n",
    "            # Fix .vhdr references\n",
    "            with open(vhdr_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            with open(vhdr_path, 'w', encoding='utf-8') as f:\n",
    "                for line in lines:\n",
    "                    if line.startswith('DataFile='):\n",
    "                        f.write(f'DataFile={basename}.eeg\\n')\n",
    "                    elif line.startswith('MarkerFile='):\n",
    "                        f.write(f'MarkerFile={basename}.vmrk\\n')\n",
    "                    else:\n",
    "                        f.write(line)\n",
    "\n",
    "            # Load raw data, specifying eog as empty list and misc channels\n",
    "            raw = mne.io.read_raw_brainvision(\n",
    "                vhdr_path,\n",
    "                preload=True,\n",
    "                eog=[],\n",
    "                misc=misc_chs\n",
    "            )\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            df = raw.to_data_frame()\n",
    "\n",
    "            # Add annotation column, default ''\n",
    "            df['annotation'] = ''\n",
    "            if raw.annotations and len(raw.annotations) > 0:\n",
    "                for onset, duration, desc in zip(raw.annotations.onset,\n",
    "                                                 raw.annotations.duration,\n",
    "                                                 raw.annotations.description):\n",
    "                    mask = (df['time'] >= onset) & (df['time'] < onset + duration)\n",
    "                    df.loc[mask, 'annotation'] = desc\n",
    "\n",
    "            # Save combined CSV\n",
    "            out_csv = os.path.join(output_path, f'{basename}.csv')\n",
    "            df.to_csv(out_csv, index=False)\n",
    "            print(f'Saved combined CSV: {out_csv}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f5df1",
   "metadata": {},
   "source": [
    "## Extend annotations for all rows and clean annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e039ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated annotations in: ./csv_output/Person1Recording1.csv\n",
      "Updated annotations in: ./csv_output/Person1Recording2.csv\n",
      "Updated annotations in: ./csv_output/Person1Recording3.csv\n",
      "Updated annotations in: ./csv_output/Person2Recording1.csv\n",
      "Updated annotations in: ./csv_output/Person2Recording2.csv\n",
      "Updated annotations in: ./csv_output/Person2Recording3.csv\n",
      "Updated annotations in: ./csv_output/Person3Recording1.csv\n",
      "Updated annotations in: ./csv_output/Person3Recording2.csv\n",
      "Updated annotations in: ./csv_output/Person3Recording3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_13556\\794490201.py:7: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated annotations in: ./csv_output/Person4Recording1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_13556\\794490201.py:7: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated annotations in: ./csv_output/Person4Recording2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_13556\\794490201.py:7: DtypeWarning: Columns (36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated annotations in: ./csv_output/Person5Recording4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_13556\\794490201.py:7: DtypeWarning: Columns (36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated annotations in: ./csv_output/Person6Recording4.csv\n",
      "Updated annotations in: ./csv_output/Person6Recording5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_13556\\794490201.py:7: DtypeWarning: Columns (36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated annotations in: ./csv_output/Person6Recording6.csv\n",
      "Updated annotations in: ./csv_output/Person7Recording7.csv\n",
      "Updated annotations in: ./csv_output/Person8Recording8.csv\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd, numpy as np\n",
    "\n",
    "for file in sorted(os.listdir(output_path)):\n",
    "    if not file.endswith('.csv'):\n",
    "        continue\n",
    "    csv_file = os.path.join(output_path, file)\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Replace empty strings with NaN for ffill\n",
    "    df['annotation'] = df['annotation'].replace('', np.nan)\n",
    "    # Remove 'Stimulus/' prefix\n",
    "    df['annotation'] = df['annotation'].str.replace(r'^Stimulus/', '', regex=True)\n",
    "    # Forward-fill\n",
    "    df['annotation'] = df['annotation'].ffill()\n",
    "    df['annotation'] = df['annotation'].fillna('')\n",
    "\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f'Updated annotations in: {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c96a739",
   "metadata": {},
   "source": [
    "## Add columns for Person and Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5c544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Person and Recording columns in: ./csv_output/Person1Recording1.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person1Recording2.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person1Recording3.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person2Recording1.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person2Recording2.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person2Recording3.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person3Recording1.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person3Recording2.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person3Recording3.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person4Recording1.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person4Recording2.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person5Recording4.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person6Recording4.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person6Recording5.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person6Recording6.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person7Recording7.csv\n",
      "Added Person and Recording columns in: ./csv_output/Person8Recording8.csv\n"
     ]
    }
   ],
   "source": [
    "import os, re, pandas as pd\n",
    "\n",
    "for file in sorted(os.listdir(output_path)):\n",
    "    if not file.endswith('.csv'):\n",
    "        continue\n",
    "    csv_file = os.path.join(output_path, file)\n",
    "    m = re.match(r'Person(\\d+)Recording(\\d+)\\.csv$', file)\n",
    "    if not m:\n",
    "        print(f\"Filename does not match pattern: {file}\")\n",
    "        continue\n",
    "    person_num, recording_num = map(int, m.groups())\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df.insert(0, 'Recording', recording_num)\n",
    "    df.insert(0, 'Person',    person_num)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f'Added Person and Recording columns in: {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5827f",
   "metadata": {},
   "source": [
    "## Remove New Segment, START, END annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf41395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed unwanted annotations in: ./csv_output/Person1Recording1.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person1Recording2.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person1Recording3.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person2Recording1.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person2Recording2.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person2Recording3.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person3Recording1.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person3Recording2.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person3Recording3.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person4Recording1.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person4Recording2.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person5Recording4.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person6Recording4.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person6Recording5.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person6Recording6.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person7Recording7.csv\n",
      "Removed unwanted annotations in: ./csv_output/Person8Recording8.csv\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd\n",
    "\n",
    "unwanted = ['New Segment/', 'START', 'END']\n",
    "for file in sorted(os.listdir(output_path)):\n",
    "    if not file.endswith('.csv'):\n",
    "        continue\n",
    "    csv_file = os.path.join(output_path, file)\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    df = df[~df['annotation'].isin(unwanted)]\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f'Removed unwanted annotations in: {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d88e11",
   "metadata": {},
   "source": [
    "## Replace lost-sample marker with REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c0c3cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oppdatert annotering i Person1Recording1.csv\n",
      "Oppdatert annotering i Person1Recording2.csv\n",
      "Oppdatert annotering i Person1Recording3.csv\n",
      "Oppdatert annotering i Person2Recording1.csv\n",
      "Oppdatert annotering i Person2Recording2.csv\n",
      "Oppdatert annotering i Person2Recording3.csv\n",
      "Oppdatert annotering i Person3Recording1.csv\n",
      "Oppdatert annotering i Person3Recording2.csv\n",
      "Oppdatert annotering i Person3Recording3.csv\n",
      "Oppdatert annotering i Person4Recording1.csv\n",
      "Oppdatert annotering i Person4Recording2.csv\n",
      "Oppdatert annotering i Person5Recording4.csv\n",
      "Oppdatert annotering i Person6Recording4.csv\n",
      "Oppdatert annotering i Person6Recording5.csv\n",
      "Oppdatert annotering i Person6Recording6.csv\n",
      "Oppdatert annotering i Person7Recording7.csv\n",
      "Oppdatert annotering i Person8Recording8.csv\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd\n",
    "\n",
    "for fname in sorted(os.listdir(output_path)):\n",
    "    if not fname.endswith('.csv'):\n",
    "        continue\n",
    "    fullpath = os.path.join(output_path, fname)\n",
    "    df = pd.read_csv(fullpath)\n",
    "\n",
    "    df['annotation'] = df['annotation'].replace(\n",
    "        'New Segment/LostSamples: 1', 'REST'\n",
    "    )\n",
    "    df.to_csv(fullpath, index=False)\n",
    "    print(f\"Oppdatert annotering i {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed795ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc91a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ecbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8555e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8deb12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "836fdac3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 1  -----------------------------\n",
    "# Load all CSVs into a single DataFrame\n",
    "import os, pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from collections import Counter\n",
    "\n",
    "csv_dir   = './csv_output'\n",
    "csv_files = [os.path.join(csv_dir, f) for f in os.listdir(csv_dir)\n",
    "             if f.endswith('.csv')]\n",
    "df        = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows from {len(csv_files)} files.\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6bc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 2  -----------------------------\n",
    "# Filter annotations, drop Aux, keep only relevant EEG channels\n",
    "imagery = True         # False ➜ drop Imagery trials\n",
    "use_aux = False        # True  ➜ keep Aux1/Aux2\n",
    "\n",
    "if not imagery:\n",
    "    df = df[~df['annotation'].str.contains('Imagery', case=False, na=False)]\n",
    "\n",
    "aux_cols = ['Aux1', 'Aux2']\n",
    "if not use_aux:\n",
    "    df = df.drop(columns=[c for c in aux_cols if c in df.columns])\n",
    "\n",
    "eeg_features = (['Person','Recording','time'] +\n",
    "    ['Fp1','Fz','F3','F7','FT9','FC5','FC1','C3','T7','TP9','CP5','CP1',\n",
    "     'Pz','P3','P7','O1','Oz','O2','P4','P8','TP10','CP6','CP2','Cz','C4',\n",
    "     'T8','FT10','FC6','FC2','F4','F8','Fp2'])\n",
    "if use_aux:\n",
    "    eeg_features += aux_cols\n",
    "\n",
    "cols_to_keep = [c for c in eeg_features + ['annotation'] if c in df.columns]\n",
    "df = df[cols_to_keep]\n",
    "\n",
    "print(\"Shape after filtering:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 3  -----------------------------\n",
    "# Build feature matrix X, encoded y, and group labels\n",
    "feature_cols = [c for c in df.columns\n",
    "                if c not in ('Person','Recording','time','annotation')]\n",
    "X = df[feature_cols].values.astype(np.float32)\n",
    "\n",
    "le    = LabelEncoder()\n",
    "y_enc = le.fit_transform(df['annotation'].values)\n",
    "groups = (df['Person'].astype(str) + '_' +\n",
    "          df['Recording'].astype(str)).values\n",
    "\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 4  -----------------------------\n",
    "# Stratified Group Shuffle → train / val / test indices\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def strat_group_split(y, groups, test_size=0.2, val_size=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Returnerer rad-indekser for train / val / test\n",
    "    med både stratifisering og gruppesammenheng.\n",
    "    \"\"\"\n",
    "    uniq_g, inv = np.unique(groups, return_inverse=True)\n",
    "    # Majoritetsklasse per gruppe (proxy-label til stratifisering)\n",
    "    maj = np.array([np.bincount(y[inv == i]).argmax() for i in range(len(uniq_g))])\n",
    "\n",
    "    # ---- trekk test-grupper -------------------------------------\n",
    "    sss = StratifiedShuffleSplit(n_splits=1,\n",
    "                                 test_size=test_size,\n",
    "                                 random_state=seed)\n",
    "    g_trainval, g_test = next(sss.split(uniq_g, maj))\n",
    "\n",
    "    # ---- trekk val-grupper fra resten ---------------------------\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1,\n",
    "                                  test_size=val_size,\n",
    "                                  random_state=seed)\n",
    "    g_train, g_val = next(sss2.split(uniq_g[g_trainval], maj[g_trainval]))\n",
    "\n",
    "    g_train = uniq_g[g_trainval][g_train]\n",
    "    g_val   = uniq_g[g_trainval][g_val]\n",
    "    g_test  = uniq_g[g_test]\n",
    "\n",
    "    idx_tr = np.where(np.isin(groups, g_train))[0]\n",
    "    idx_va = np.where(np.isin(groups, g_val))[0]\n",
    "    idx_te = np.where(np.isin(groups, g_test))[0]\n",
    "    return idx_tr, idx_va, idx_te\n",
    "\n",
    "# kall funksjonen\n",
    "train_idx, val_idx, test_idx = strat_group_split(y_enc, groups)\n",
    "\n",
    "print(\"Train:\", Counter(y_enc[train_idx]))\n",
    "print(\"Val  :\", Counter(y_enc[val_idx]))\n",
    "print(\"Test :\", Counter(y_enc[test_idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 5  -----------------------------\n",
    "# MNE filtering  →  1-s z-scored windows (50 % overlap)\n",
    "import mne\n",
    "from mne.filter import filter_data, notch_filter\n",
    "\n",
    "sfreq, win_s = 256, 1.0\n",
    "win, step    = int(win_s*sfreq), int(win_s*sfreq)//2\n",
    "\n",
    "def preprocess(rec_df):\n",
    "    dat = rec_df[feature_cols].values.T\n",
    "    dat = notch_filter(dat, sfreq, freqs=[50], verbose=False)\n",
    "    dat = filter_data(dat, sfreq, 8, 30, verbose=False)\n",
    "    return dat - dat.mean(0, keepdims=True)  # CAR\n",
    "\n",
    "def make_windows(idx_subset):\n",
    "    Xw, yw = [], []\n",
    "    for g in np.unique(groups[idx_subset]):\n",
    "        df_rec = df[groups==g].sort_values('time')\n",
    "        if len(df_rec) < sfreq*3: continue\n",
    "        arr, labels = preprocess(df_rec), df_rec['annotation'].values\n",
    "        for s in range(0, arr.shape[1]-win+1, step):\n",
    "            seg_lab = labels[s:s+win]\n",
    "            if (seg_lab==seg_lab[0]).mean() >= .8:\n",
    "                seg = arr[:, s:s+win]\n",
    "                seg = (seg-seg.mean(1,keepdims=True))/ (seg.std(1,keepdims=True)+1e-6)\n",
    "                Xw.append(seg); yw.append(seg_lab[0])\n",
    "    return np.stack(Xw), np.array(yw)\n",
    "\n",
    "X_tr, y_tr = make_windows(train_idx)\n",
    "X_va, y_va = make_windows(val_idx)\n",
    "X_te, y_te = make_windows(test_idx)\n",
    "print(\"Windows:\", X_tr.shape, X_va.shape, X_te.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee20149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 6  -----------------------------\n",
    "# Feature transformer: Covariances→TangentSpace + Band-power\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from scipy.signal import welch\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "bands = [(0.5,4),(4,8),(8,12),(13,30)]\n",
    "def bandpow(seg):\n",
    "    f,p = welch(seg, sfreq, nperseg=sfreq//2)\n",
    "    return np.concatenate([p[:,(f>=lo)&(f<=hi)].mean(1) for lo,hi in bands])\n",
    "\n",
    "class BandPower(BaseEstimator, TransformerMixin):\n",
    "    def fit(self,X,y=None): return self\n",
    "    def transform(self,X):  return np.array([bandpow(x) for x in X])\n",
    "\n",
    "cov_ts = Pipeline([('cov', Covariances('oas')),\n",
    "                   ('ts',  TangentSpace(metric='riemann'))])\n",
    "\n",
    "feat_pipe = Pipeline([\n",
    "    ('features', ColumnTransformer(\n",
    "        [('ts', cov_ts, slice(None)),\n",
    "         ('bp', BandPower(), slice(None))])),\n",
    "    ('sc', StandardScaler())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce71a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 7  -----------------------------\n",
    "# Balanced batch generator + EEGNet training\n",
    "import tensorflow as tf, math, random\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "n_classes = len(le.classes_)\n",
    "def EEGNet(nb_classes, Chans, Samples, dropout=0.4):\n",
    "    from tensorflow.keras.layers import (Input, Conv2D, DepthwiseConv2D,\n",
    "         SeparableConv2D, BatchNormalization, Activation, AveragePooling2D,\n",
    "         SpatialDropout2D, Flatten, Dense)\n",
    "    inp = Input((Chans,Samples,1))\n",
    "    x = Conv2D(8,(1,64),padding='same',use_bias=False)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = DepthwiseConv2D((Chans,1),depth_multiplier=2,use_bias=False)(x)\n",
    "    x = BatchNormalization()(x); x=Activation('elu')(x)\n",
    "    x = AveragePooling2D((1,4))(x); x=SpatialDropout2D(dropout)(x)\n",
    "    x = SeparableConv2D(16,(1,16),padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x); x=Activation('elu')(x)\n",
    "    x = AveragePooling2D((1,8))(x); x=SpatialDropout2D(dropout)(x)\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(nb_classes, activation='softmax')(x)\n",
    "    return tf.keras.Model(inp,out)\n",
    "\n",
    "class BalBatch(Sequence):\n",
    "    def __init__(self,X,y,batch=64):\n",
    "        self.X, self.y = X, np.array(le.transform(y))\n",
    "        self.b, self.cls = batch, np.unique(self.y)\n",
    "        self.idxs = {c:np.where(self.y==c)[0] for c in self.cls}\n",
    "    def __len__(self): return math.ceil(len(self.y)/self.b)\n",
    "    def __getitem__(self,i):\n",
    "        per = max(1,self.b//len(self.cls)); idx=[]\n",
    "        for c in self.cls: idx+=random.sample(self.idxs[c].tolist(),per)\n",
    "        random.shuffle(idx)\n",
    "        return (self.X[idx,...,np.newaxis],\n",
    "                to_categorical(self.y[idx], n_classes))\n",
    "\n",
    "train_gen = BalBatch(X_tr, y_tr, 64)\n",
    "val_gen   = BalBatch(X_va, y_va, 64)\n",
    "\n",
    "model = EEGNet(n_classes, X_tr.shape[1], X_tr.shape[2])\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)\n",
    "\n",
    "model.fit(train_gen, epochs=60, validation_data=val_gen,\n",
    "          callbacks=[es], verbose=1)\n",
    "\n",
    "X_te4D = X_te[..., np.newaxis]\n",
    "print(\"Test accuracy:\",\n",
    "      model.evaluate(X_te4D,\n",
    "      to_categorical(le.transform(y_te), n_classes), verbose=0)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 8  -----------------------------\n",
    "# Confusion matrix on test-set\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = model.predict(X_te4D, verbose=0).argmax(1)\n",
    "y_true = le.transform(y_te)\n",
    "\n",
    "print(\"\\nTest classification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true, y_pred,\n",
    "    display_labels=le.classes_, cmap='Blues',\n",
    "    xticks_rotation=45, ax=ax, colorbar=False\n",
    ")\n",
    "ax.set_title(\"EEGNet – Confusion Matrix (Test)\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 9  -----------------------------\n",
    "# Lagre alle artefakter for sanntids-inferens\n",
    "# --------------------------------------------------------------------\n",
    "import os, json, numpy as np, tensorflow as tf\n",
    "save_dir = \"./saved_artifacts\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 1) TF-modell\n",
    "model.save(os.path.join(save_dir, \"EEGNet_MI\"))\n",
    "\n",
    "# 2) Etiketter  →  json + npy\n",
    "lbl_json = os.path.join(save_dir, \"label_classes.json\")\n",
    "lbl_npy  = os.path.join(save_dir, \"label_classes.npy\")\n",
    "with open(lbl_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(le.classes_.tolist(), f, indent=2)\n",
    "np.save(lbl_npy, le.classes_)                 # back-compat\n",
    "print(\"✅ etiketter →\", lbl_json, \"og\", lbl_npy)\n",
    "\n",
    "# 3) Kanal-rekkefølge\n",
    "with open(os.path.join(save_dir, \"eeg_channels.json\"), \"w\") as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "\n",
    "# 4) Pre-proc-meta\n",
    "meta_info = {\n",
    "    \"sfreq\": sfreq, \"window_s\": win_s, \"window_len\": win,\n",
    "    \"step_len\": step, \"overlap\": 0.5,\n",
    "    \"zscore\": True, \"bandpass\": [8, 30], \"notch\": 50\n",
    "}\n",
    "with open(os.path.join(save_dir, \"preproc_meta.json\"), \"w\") as f:\n",
    "    json.dump(meta_info, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c552e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ea923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8dc018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0aeecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b7f00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3d7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66a9b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b16a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b123cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835618f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7959d24",
   "metadata": {},
   "source": [
    "# NY MODELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ac560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 1 -----------------------------\n",
    "import os, pandas as pd, numpy as np\n",
    "\n",
    "csv_dir = './csv_output'\n",
    "csv_files = [os.path.join(csv_dir, f) for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "df = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "\n",
    "# Behold kun REST og IMAGERY\n",
    "df = df[df['annotation'].str.contains('REST|IMAGERY', case=False, na=False)]\n",
    "\n",
    "# Lag binær annotasjon\n",
    "df['label'] = np.where(df['annotation'].str.contains('IMAGERY'), 'IMAGERY', 'REST')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Klassefordeling:\", df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 2 -----------------------------\n",
    "epoch_duration = 4   # kortere epochs gir mer data\n",
    "fs = 500             # samplingfrekvens\n",
    "\n",
    "def create_epochs(df, epoch_duration, fs):\n",
    "    epochs, labels = [], []\n",
    "    n_samples_epoch = epoch_duration * fs\n",
    "    for label in ['REST', 'IMAGERY']:\n",
    "        df_label = df[df['label'] == label]\n",
    "        n_epochs = len(df_label) // n_samples_epoch\n",
    "        for i in range(n_epochs):\n",
    "            epoch = df_label.iloc[i*n_samples_epoch:(i+1)*n_samples_epoch]\n",
    "            epoch_data = epoch.drop(columns=['Person','Recording','time','annotation','label']).values.T\n",
    "            epochs.append(epoch_data)\n",
    "            labels.append(label)\n",
    "    return np.array(epochs), np.array(labels)\n",
    "\n",
    "epochs, labels = create_epochs(df, epoch_duration, fs)\n",
    "print(f'Epochs shape: {epochs.shape}, labels shape: {labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 3 -----------------------------\n",
    "cut_seconds = 1\n",
    "cut_samples = cut_seconds * fs\n",
    "\n",
    "epochs = epochs[:, :, cut_samples:]\n",
    "print(\"Shape etter kutt:\", epochs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578cca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 4 -----------------------------\n",
    "# I stedet for 0.5–40 Hz, prøv f.eks. 8–30 Hz (alfa + beta-bånd)\n",
    "epochs_filtered = mne.filter.filter_data(\n",
    "    epochs, sfreq=fs, l_freq=8, h_freq=30, verbose=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb71cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 5 -----------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    epochs_filtered, labels, test_size=0.15, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.15, stratify=y_trainval, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d31051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 6 -----------------------------\n",
    "from mne.decoding import CSP\n",
    "\n",
    "csp = CSP(n_components=8, reg=None, log=True, norm_trace=False)\n",
    "\n",
    "X_train_csp = csp.fit_transform(X_train, y_train)\n",
    "X_val_csp = csp.transform(X_val)\n",
    "X_test_csp = csp.transform(X_test)\n",
    "\n",
    "print(f'CSP train: {X_train_csp.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69daaab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 7 -----------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_csp)\n",
    "X_val_scaled = scaler.transform(X_val_csp)\n",
    "X_test_scaled = scaler.transform(X_test_csp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccbef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Ny Block 7b -----------------------------\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Original training label distribution:\", Counter(y_train))\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_balanced, y_train_balanced = rus.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Balanced training label distribution:\", Counter(y_train_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 8 -----------------------------\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(svm, param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Beste parametre:\", grid.best_params_)\n",
    "print(\"Beste CV accuracy:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 9 -----------------------------\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "model = grid.best_estimator_\n",
    "\n",
    "# Valideringsresultater\n",
    "y_val_pred = model.predict(X_val_scaled)\n",
    "print(\"Valideringsresultater:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(\"Valideringsnøyaktighet:\", accuracy_score(y_val, y_val_pred))\n",
    "\n",
    "# Testresultater\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "print(\"Testresultater:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"Testnøyaktighet:\", accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eaf834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Block 10 (Confusion Matrix) -----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred, labels=['IMAGERY', 'REST'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['IMAGERY', 'REST'], yticklabels=['IMAGERY', 'REST'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Test Set)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c962b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f954c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52387a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257da98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc33c5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e2cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a38b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f14756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b329e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2262d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e14910f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552dbc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64166970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Block 1: Last inn datasett, fjern unødvendige kanaler, binær-annotasjon -----------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For reproducerbarhet\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1) Les inn alle CSV-filer\n",
    "csv_dir = './csv_output'\n",
    "csv_files = [os.path.join(csv_dir, f) for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "df = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "\n",
    "# 2) Standardiser annotation-tekst\n",
    "df['annotation'] = df['annotation'].str.upper().str.strip()\n",
    "\n",
    "# 3) Identifiser EEG-kanaler: ta alle kolonner unntatt metadata, fjern AUX og DIR\n",
    "metadata_cols = ['Person', 'Recording', 'time', 'annotation']\n",
    "all_channels  = [c for c in df.columns if c not in metadata_cols]\n",
    "eeg_channels  = [c for c in all_channels if ('AUX' not in c.upper()) and ('DIR' not in c.upper())]\n",
    "\n",
    "# 4) Filtrer bort uønskede kolonner, behold kun EEG-signaler + metadata\n",
    "df = df[metadata_cols + eeg_channels].copy()\n",
    "\n",
    "# 5) Binær-annotasjon: behold kun REST eller IMAGERY-varianter, lag label\n",
    "df = df[df['annotation'].str.contains('REST|IMAGERY', na=False)].reset_index(drop=True)\n",
    "df['label'] = np.where(df['annotation'].str.contains('IMAGERY'), 'IMAGERY', 'REST')\n",
    "\n",
    "# 6) Sjekk datasett og klassedistribusjon\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"Antall EEG-kanaler:\", len(eeg_channels))\n",
    "print(\"Klassefordeling:\\n\", df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Block 2: Notch- og båndpass-filtrering -----------------------------\n",
    "import mne\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Opprett MNE RawArray fra DataFrame\n",
    "sfreq = 500.0  # samplingfrekvens\n",
    "info = mne.create_info(ch_names=eeg_channels, sfreq=sfreq, ch_types='eeg')\n",
    "raw = mne.io.RawArray(df[eeg_channels].T.values, info)\n",
    "\n",
    "# 2) Notch-filter ved 50 Hz for å fjerne strømnettstøy\n",
    "#    - FIR-notching med smal båndbredde rundt 50 Hz\n",
    "raw.notch_filter(freqs=50., fir_design='firwin', verbose=True)\n",
    "\n",
    "# 3) Båndpass 1–40 Hz for å fange de viktigste EEG-båndene (drift ↔ gamma)\n",
    "#    - 1 Hz high-pass for å fjerne DC-drift\n",
    "#    - 40 Hz low-pass for å undertrykke høyfrekvent støy\n",
    "raw.filter(l_freq=1.0, h_freq=40.0, fir_design='firwin', verbose=True)\n",
    "\n",
    "# 4) Les det filtrerte signalet tilbake til en DataFrame\n",
    "df_filtered = pd.DataFrame(raw.get_data().T, columns=eeg_channels)\n",
    "\n",
    "# 5) Sjekk at filtreringen er brukt\n",
    "print(raw.info)                 # skal vise 32 EEG-kanaler @ 500 Hz\n",
    "print(\"First values (filtered):\")\n",
    "print(df_filtered.iloc[:5, :5])  # vis et lite utdrag av de filtrerte dataene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Block 3 -----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Block 4 (oppdatert): Epoching med gruppelabels -----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_clean = raw.get_data().T    \n",
    "persons    = df['Person'].values       \n",
    "recs       = df['Recording'].values    \n",
    "labels_all = df['label'].values        \n",
    "\n",
    "sfreq       = raw.info['sfreq']\n",
    "epoch_samps = int(4 * sfreq)\n",
    "step        = epoch_samps\n",
    "\n",
    "# 3) Lag epoker, majoritets-label og gruppelabel per epoke\n",
    "X_epochs, y_epochs, groups = [], [], []\n",
    "for start in range(0, len(data_clean) - epoch_samps + 1, step):\n",
    "    end = start + epoch_samps\n",
    "    seg = data_clean[start:end]    \n",
    "    baseline = seg[:int(0.5*sfreq), :].mean(axis=0, keepdims=True)\n",
    "    seg_bc = seg - baseline\n",
    "    X_epochs.append(seg_bc.T)\n",
    "    lab = pd.Series(labels_all[start:end]).mode()[0]\n",
    "    y_epochs.append(lab)\n",
    "    groups.append(f\"{persons[start]}__{recs[start]}\")\n",
    "\n",
    "X_epochs = np.array(X_epochs)    # (n_epochs, n_channels, n_times)\n",
    "y_epochs = np.array(y_epochs)\n",
    "groups    = np.array(groups)\n",
    "\n",
    "# 4) Sjekk\n",
    "import pandas as pd\n",
    "print(\"Epochs shape:\", X_epochs.shape)\n",
    "print(\"Label-fordeling:\\n\", pd.Series(y_epochs).value_counts())\n",
    "print(\"Antall unike grupper (Person__Recording):\", len(np.unique(groups)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b1aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Block 5 (oppdatert): Group-based split i train/val/test -----------------------------\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Først: train vs (val+test)\n",
    "gss1 = GroupShuffleSplit(n_splits=1, test_size=0.30, random_state=42)\n",
    "train_idx, vt_idx = next(gss1.split(X_epochs, y_epochs, groups))\n",
    "\n",
    "# 2) Deretter: val vs test på det resterende\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.50, random_state=42)\n",
    "val_idx_rel, test_idx_rel = next(gss2.split(X_epochs[vt_idx], y_epochs[vt_idx], groups[vt_idx]))\n",
    "val_idx  = vt_idx[val_idx_rel]\n",
    "test_idx = vt_idx[test_idx_rel]\n",
    "\n",
    "# 3) Del opp data\n",
    "X_train, y_train = X_epochs[train_idx],    y_epochs[train_idx]\n",
    "X_val,   y_val   = X_epochs[val_idx],      y_epochs[val_idx]\n",
    "X_test,  y_test  = X_epochs[test_idx],     y_epochs[test_idx]\n",
    "\n",
    "# 4) Verifiser at ingen grupper overlapper\n",
    "train_groups = set(groups[train_idx])\n",
    "val_groups   = set(groups[val_idx])\n",
    "test_groups  = set(groups[test_idx])\n",
    "assert train_groups.isdisjoint(val_groups)\n",
    "assert train_groups.isdisjoint(test_groups)\n",
    "assert val_groups.isdisjoint(test_groups)\n",
    "print(\"Ingen gruppelekkasje mellom train/val/test ✔️\")\n",
    "\n",
    "# 5) Sjekk former og fordeling\n",
    "print(\"Train shape:\", X_train.shape, \" Val shape:\", X_val.shape, \" Test shape:\", X_test.shape)\n",
    "print(\"Train distrib:\\n\", pd.Series(y_train).value_counts())\n",
    "print(\"Val distrib:\\n\",   pd.Series(y_val).value_counts())\n",
    "print(\"Test distrib:\\n\",  pd.Series(y_test).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a96481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Block 6 (alternativ): Riemann Tangent Space-features -----------------------------\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "\n",
    "# 1) Beregn kovariansmatriser for hver epoke (n_epochs, n_channels, n_channels)\n",
    "cov_est = Covariances(estimator='lwf')\n",
    "X_train_cov = cov_est.fit_transform(X_train)    # bruker kun X_train\n",
    "X_val_cov   = cov_est.transform(X_val)\n",
    "X_test_cov  = cov_est.transform(X_test)\n",
    "\n",
    "# 2) Mapper kovarianser til tangentrom\n",
    "ts = TangentSpace()\n",
    "X_train_csp = ts.fit_transform(X_train_cov, y_train)   # navner output X_train_csp\n",
    "X_val_csp   = ts.transform(X_val_cov)                  # navner output X_val_csp\n",
    "X_test_csp  = ts.transform(X_test_cov)                 # navner output X_test_csp\n",
    "\n",
    "# 3) Sjekk at formater samsvarer med resten av pipelinen\n",
    "print(\"Train Riemann shape:\", X_train_csp.shape)\n",
    "print(\"Val   Riemann shape:\", X_val_csp.shape)\n",
    "print(\"Test  Riemann shape:\", X_test_csp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Block 7: Skalering + SVM med klassevekter -----------------------------\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "# Rett import:\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Pakk Riemann‑transformerte features + SVM i pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC(kernel='rbf', class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Gjør 5‑fold stratified CV på alle Riemann‑features\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X_all = np.vstack([X_train_csp, X_val_csp, X_test_csp])\n",
    "y_all = np.hstack([y_train, y_val, y_test])\n",
    "scores = cross_val_score(pipe_full, X_all, y_all,\n",
    "                         cv=cv, scoring='f1_weighted', n_jobs=-1)\n",
    "print(f\"5‑fold mean={scores.mean():.3f}, std={scores.std():.3f}\")\n",
    "\n",
    "\n",
    "# 2) Tren på treningssettets CSP-features\n",
    "pipe.fit(X_train_csp, y_train)\n",
    "\n",
    "# 3) Evaluer på valideringssettet\n",
    "y_val_pred = pipe.predict(X_val_csp)\n",
    "print(\"Valideringsnøyaktighet:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Valideringsrapport:\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=['REST','IMAGERY']))\n",
    "\n",
    "# 4) Evaluer på testsettet\n",
    "y_test_pred = pipe.predict(X_test_csp)\n",
    "print(\"Testnøyaktighet:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Testrapport:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['REST','IMAGERY']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Block 8: Plot Confusion Matrix for Test-set -----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Definer klasserekkefølgen én gang\n",
    "labels = ['REST', 'IMAGERY']\n",
    "\n",
    "# 1) Skriv ut klassifikasjonsrapport med riktig etikett-rekkefølge\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, y_test_pred,\n",
    "    labels=labels,\n",
    "    target_names=labels\n",
    "))\n",
    "\n",
    "# 2) Beregn og plott forvirringsmatrisen med samme etiketter\n",
    "cm = confusion_matrix(y_test, y_test_pred, labels=labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel('Predicted label')\n",
    "ax.set_ylabel('True label')\n",
    "ax.set_title('Confusion Matrix (Test Set)')\n",
    "\n",
    "# Annoter hver rute med tall\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7d4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc21dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da8ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b48d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09effff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909eca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
